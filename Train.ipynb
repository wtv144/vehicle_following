{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from ts_data import TS_Data\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn as nn\n",
    "from CNN import CNN\n",
    "from metrics import nll"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "fdir = \"/home/warren/Documents/UGRA/AIM/csv_files\"\n",
    "def get_dataset(fdir, in_len,pred_len):\n",
    "    l = os.listdir(fdir)\n",
    "    datasets = [None]*len(l)\n",
    "    for i in range(len(l)):\n",
    "        temp = os.path.join(fdir,l[i])\n",
    "        datasets[i] = TS_Data(temp, in_len,pred_len)\n",
    "    dataset =  torch.utils.data.ConcatDataset(datasets)\n",
    "    return dataset\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "inlen = 8\n",
    "predlen=4\n",
    "full_dataset = get_dataset(fdir,inlen, predlen)\n",
    "train_len = int(.8* len(full_dataset))\n",
    "test_len = len(full_dataset)-train_len\n",
    "train_data, test_data = torch.utils.data.random_split(full_dataset, [train_len, test_len])\n",
    "train_dataloader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=256)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#model = CNN()\n",
    "#take out the z "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#model = nn.LSTM(2, 5,5, batch_first=True) #ensure that batch is first \n",
    "model = CNN(8,4)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "criterion = nll()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = .0001)\n",
    "epochs = 4\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "    #list of predictions is [batch size, steps - 8, ]    \n",
    "def pred_to_samples(num_samples, pred):\n",
    "        sx = torch.exp(pred[2]) #sx\n",
    "        sy = torch.exp(pred[3]) #sy\n",
    "        corr = torch.tanh(pred[4]) #corr\n",
    "        sxsy = sx*sy\n",
    "        cov = torch.FloatTensor([[sx*sx, corr*sxsy],[corr*sxsy, sy*sy]])\n",
    "        means = torch.FloatTensor([pred[0],pred[1]])\n",
    "        dist = torch.distributions.multivariate_normal.MultivariateNormal(means, cov)\n",
    "        return dist.sample((num_samples,))\n",
    "#no idea if I need to detach for these \n",
    "\n",
    "def ade_dist(pred,target):\n",
    "        return (torch.sum(torch.square(pred-target))/pred.shape[1]).item() #assume the middle dimension\n",
    "\n",
    "\n",
    "def fde_dist(pred, target):\n",
    "        return (torch.sum(torch.square(pred[-1] - target[-1]))).item() #only compare the final location of pred to final location of tgt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def valid():\n",
    "    print(\"valid\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_sum_ade = 0\n",
    "        batch_sum_fde=0\n",
    "        for idx, (inputs, labels) in enumerate(test_dataloader): #batch size, steps, etc\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            T = inputs.shape[0]\n",
    "            N = inputs.shape[1]\n",
    "            preds = model(inputs) #each step has a distribution. So take a sample of each step and then turn it into something cumulative\n",
    "\n",
    "            for b in range(T): #iterate over batches\n",
    "                curr = preds[b,:,:]\n",
    "                #iterate over curr\n",
    "                num_samples = 20\n",
    "                num_steps = curr.shape[0]\n",
    "                sample_list = []\n",
    "                for i in range(num_steps):\n",
    "                    sample_list.append(pred_to_samples(num_samples, curr[i]))\n",
    "                    #now should have a list of samples for each step\n",
    "\n",
    "                samples = torch.stack(sample_list,dim=1) #stack it along middle dim to get [num_samples, num_steps, (x,y)]\n",
    "                #now cumsum it \n",
    "                abs_samples = samples.cumsum(dim=1) #dim =1 since it is the number of steps shape is [num_samples, steps, rel pos]\n",
    "                # now take the minimum ade/fde of the absolute samples to it \n",
    "                fde_dists = []\n",
    "                ade_dists = []\n",
    "                temp_label = labels[b,:,:]\n",
    "                temp_label = temp_label.cumsum(dim=0)\n",
    "                for i in range(num_samples):\n",
    "                    #calculate the metrics for each\n",
    "                    fde_dists.append(fde_dist(abs_samples[i].to(device),temp_label))\n",
    "                    ade_dists.append(ade_dist(abs_samples[i].to(device),temp_label))\n",
    "                #now take the min\n",
    "                min_fde = min(fde_dists)\n",
    "                min_ade = min(ade_dists)\n",
    "                batch_sum_ade+=min_ade\n",
    "                batch_sum_fde+=min_fde\n",
    "            \n",
    "            \n",
    "        batch_sum_ade/= len(test_dataloader)\n",
    "        batch_sum_fde/= len(test_dataloader)\n",
    "        valid_ade.append(batch_sum_ade)\n",
    "        valid_fde.append(batch_sum_fde)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "losses = []\n",
    "valid_ade = []\n",
    "valid_fde = []\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    with torch.autograd.detect_anomaly():\n",
    "        for idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds= model(inputs)\n",
    "        #predictions are nan for some reason\n",
    "            loss  = criterion(preds, labels) \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),.001)\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    #Validation \n",
    "    valid()\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-8-ee402afdc4d5>:6: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n",
      "/home/warren/mlenv/lib/python3.8/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "valid\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    with torch.no_grad():\n",
    "        batch_sum_ade = 0\n",
    "        batch_sum_fde=0\n",
    "        for idx, (inputs, labels) in enumerate(test_dataloader): #batch size, steps, etc\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            T = inputs.shape[0]\n",
    "            N = inputs.shape[1]\n",
    "            preds = model(inputs) #each step has a distribution. So take a sample of each step and then turn it into something cumulative\n",
    "            sample_list[i] = pred_to_samples(num_samples, curr[i])\n",
    "                \n",
    "                #iterate over curr\n",
    "                num_samples = 20\n",
    "                num_steps = curr.shape[0]\n",
    "                sample_list = []*num_steps\n",
    "                for i in range(num_steps):\n",
    "                    sample_list[i] = pred_to_samples(num_samples, curr[i])\n",
    "                    #now should have a list of samples for each step\n",
    "\n",
    "                samples = torch.stack(samples,dim=1) #stack it along middle dim to get [num_samples, num_steps, (x,y)]\n",
    "                #now cumsum it \n",
    "                abs_samples = samples.cumsum(dim=1) #dim =1 since it is the number of steps shape is [num_samples, steps, rel pos]\n",
    "                # now take the minimum ade/fde of the absolute samples to it \n",
    "                fde_dists = []\n",
    "                ade_dists = []\n",
    "                for i in range(num_samples):\n",
    "                    #calculate the metrics for each\n",
    "                    fde_dists.append(fde_dist(abs_samples[i]))\n",
    "                    ade_dists.append(ade_dist(abs_samples[i]))\n",
    "                #now take the min\n",
    "                min_fde = min(fde_dists)\n",
    "                min_ade = min(ade_dists)\n",
    "                batch_sum_ade+=min_ade\n",
    "                batch_sum_fde+=min_fde\n",
    "            \n",
    "            \n",
    "        batch_sum_ade/= len(test_dataloader)\n",
    "        batch_sum_fde/= len(test_dataloader)\n",
    "        print(\"done 1\")\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(losses)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3.1433467864990234, 3.042160749435425, 2.7600347995758057, 2.7089853286743164, 2.619258165359497, 2.7567954063415527, 2.5506069660186768, 2.5198793411254883, 2.678232192993164, 2.419008731842041, 2.6152989864349365, 2.4690394401550293, 2.4687514305114746, 2.3994381427764893, 2.3469181060791016, 2.336169719696045, 2.3863279819488525, 2.2950239181518555, 2.330838203430176, 2.239394187927246, 2.2979226112365723, 2.2310032844543457, 2.280668258666992, 2.1629772186279297, 2.1138134002685547, 2.1755824089050293, 2.104759693145752, 2.0242767333984375, 2.0752296447753906, 2.034060001373291, 2.0100159645080566, 1.9700212478637695, 2.029034376144409, 1.9620453119277954, 1.899348497390747, 1.864247441291809, 1.973174810409546, 1.8742237091064453, 1.817836046218872, 1.8262577056884766, 2.585263729095459, 1.801393985748291, 1.7678054571151733, 1.7637109756469727, 1.7639069557189941, 1.7116169929504395, 1.726269245147705, 1.6954078674316406, 1.649776816368103, 1.8043162822723389, 1.6419544219970703, 1.6139352321624756, 1.6023963689804077, 1.6350774765014648, 1.578428864479065, 1.7162331342697144, 1.5353926420211792, 1.5371603965759277, 1.5238226652145386, 1.4638621807098389, 1.506964087486267, 1.4892032146453857, 1.4344274997711182, 1.425024390220642, 1.3989070653915405, 1.4383931159973145, 1.349912405014038, 1.359798789024353, 1.3104913234710693, 1.3244476318359375, 1.3585976362228394, 1.299757719039917, 1.4586929082870483, 1.2070865631103516, 1.256664752960205, 1.1849775314331055, 1.209306240081787, 1.2173869609832764, 1.2391808032989502, 1.261894702911377, 1.119842767715454, 1.1670889854431152, 1.0719358921051025, 1.3178904056549072, 1.047481894493103, 1.1133296489715576, 1.011420726776123, 1.1545140743255615, 1.0277323722839355, 0.9752278923988342, 1.0229594707489014, 0.9959940910339355, 0.9981709122657776, 1.0076379776000977, 1.0542335510253906, 0.8570623397827148, 0.9584606885910034, 0.8710150122642517, 0.8628263473510742, 0.8040404319763184, 0.9722409844398499, 0.804863691329956, 0.9140315055847168, 0.8912752866744995, 0.7148892879486084, 0.7841553688049316, 0.7219014763832092, 0.7140411734580994, 0.9447128772735596, 0.8115158677101135, 0.6993393898010254, 1.1163111925125122, 0.7298622131347656, 0.7105069756507874, 0.7406579256057739, 0.6515821218490601, 0.5859576463699341, 0.6216281056404114, 0.5475138425827026, 0.5274637937545776, 0.47114861011505127, 0.451020747423172, 0.5831235647201538, 0.5290157794952393, 0.4597054719924927, 0.5540384650230408, 0.48245730996131897, 0.3825676143169403, 0.4197303354740143, 0.5701653957366943, 0.44521021842956543, 0.9001091718673706, 0.5744427442550659, 0.2906198501586914, 0.9690949320793152, 0.2939518690109253, 0.4058116376399994, 0.3374786078929901, 0.3675452768802643, 0.26224285364151, 0.19523313641548157, 0.37287455797195435, 0.2513609826564789, 0.2829585373401642, 0.2813293933868408]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Issues\n",
    "1. Had to change prediction length to the same as the input length for LSTM\n",
    "2. Do i need to detach or is calling item() sufficient\n",
    "3. The [batch_size, 8,5] means that each step has a distribution. So each x,y in a sequence of steps is from a different distribution and then make absolute\n",
    "4. Making a CNN compatible with time series "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assume take in 8 time steps, and predict 4 steps\n",
    "1. Input into model, and model will return distribution parameters\n",
    "2. Take a loss with 4 time steps and compare to distribution\n",
    "3. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Model predicts distribution, parameters, mux,muy sx,sy, rho\n",
    "2. creat mean and cov matrix to make distribution. Torch multivariate_norm. tanh for coeff, exp for sx \n",
    "3. generate samples from distribution, 20 trajecctories . NOTE: each step has its own distribution. So for one trajectory, use 8 distributions\n",
    "4. Convert samples from relative to absolute\n",
    "4. Compare samples to the target one, take the minimum \n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Issue with the prediction length, set it to the same as the input length. Changed the [:,1] to [...,1] etc"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for b in range(num_batches):\n",
    "    curr = preds[b,:,:]\n",
    "    #iterate over curr\n",
    "    num_samples = 20\n",
    "    num_steps = curr.shape[0]\n",
    "    sample_list = []*num_steps\n",
    "    for i in range(num_steps):\n",
    "        sample_list[i] = pred_to_samples(num_samples, curr[i])\n",
    "        #now should have a list of samples for each step\n",
    "        \n",
    "    samples = torch.stack(samples,dim=1) #stack it along middle dim to get [num_samples, num_steps, (x,y)]\n",
    "    #now cumsum it \n",
    "    abs_samples = samples.cumsum(dim=1) #dim =1 since it is the number of steps shape is [num_samples, steps, rel pos]\n",
    "    # now take the minimum ade/fde of the absolute samples to it \n",
    "    fde_dists = []\n",
    "    ade_dists = []\n",
    "    for i in range(num_samples):\n",
    "        #calculate the metrics for each\n",
    "        fde_dists.append(fde_dist(abs_samples[i]))\n",
    "        ade_dists.append(ade_dist(abs_samples[i]))\n",
    "    #now take the min\n",
    "    min_fde = min(fde_dists)\n",
    "    min_ade = min(ade_dists)\n",
    "    \n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = torch.rand((5,4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "dist = torch.distributions.multivariate_normal.MultivariateNormal(torch.FloatTensor([0,0]), torch.eye(2))\n",
    "samples = dist.sample((20))\n",
    "samples.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20, 8, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "samples[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "s1 = pred_to_samples(20,torch.FloatTensor([1,0,1,1,0]))\n",
    "s2 = pred_to_samples(20,torch.FloatTensor([1,0,1,1,0]))\n",
    "print(s1[0])\n",
    "print(s2[0])\n",
    "a = [s1,s2]\n",
    "print(len(a))\n",
    "print(len(a[0]))\n",
    "a = torch.stack(a, dim =1 )\n",
    "print(a.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-2.9822, -2.4117])\n",
      "tensor([1.0272, 3.2640])\n",
      "2\n",
      "20\n",
      "torch.Size([20, 2, 2])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "a = torch.rand(5,2)\n",
    "b = torch.rand(5,2)\n",
    "print(a)\n",
    "print(b)\n",
    "torch.stack([a,b],dim=1).shape\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.4471, 0.8284],\n",
      "        [0.6522, 0.4346],\n",
      "        [0.4059, 0.5153],\n",
      "        [0.3018, 0.9224],\n",
      "        [0.3225, 0.8794]])\n",
      "tensor([[0.3184, 0.6030],\n",
      "        [0.8760, 0.7945],\n",
      "        [0.6209, 0.9229],\n",
      "        [0.0354, 0.9280],\n",
      "        [0.3968, 0.6559]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a = (torch.rand((20,8,2))).cumsum(dim=1)\n",
    "b = (torch.rand((20,8,2))).cumsum(dim=1)\n",
    "ade_dist(a,b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#replace L by H\n",
    "\n",
    "a = torch.rand((4,2))\n",
    "print(a)\n",
    "a.cumsum(dim=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.8299, 0.1746],\n",
      "        [0.7730, 0.9449],\n",
      "        [0.9312, 0.6769],\n",
      "        [0.4362, 0.0676]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.8299, 0.1746],\n",
       "        [1.6029, 1.1195],\n",
       "        [2.5342, 1.7964],\n",
       "        [2.9704, 1.8640]])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0df133046154da9ae7688aa7430ed73ae5cc3be12043136e904e8c0717ac54e0"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('mlenv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}